{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wycrTurhfWtm",
        "outputId": "cea66265-f022-4620-c64c-fd4cafa5674a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¬ LLM Medical Ontology Evaluation Tool\n",
            "==================================================\n",
            "This notebook evaluates LLM performance on hierarchical medical ontology relationships.\n",
            "\n",
            "To use:\n",
            "1. Upload your files: test.jsonl, entity_lexicon.json, answers.txt\n",
            "2. Run evaluate_llm_answers() to get evaluation results\n",
            "3. Run show_data_info() to see dataset information\n",
            "\n",
            "Ready to start! Run the functions below:\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Notebook: LLM Evaluation for Hierarchical Medical Ontology\n",
        "# Upload your files: test.jsonl, entity_lexicon.json, answers.txt to the same folder\n",
        "\n",
        "# Install required packages\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from typing import List, Dict, Tuple\n",
        "from google.colab import files\n",
        "\n",
        "def upload_files():\n",
        "    \"\"\"Upload files to Colab\"\"\"\n",
        "    print(\"Please upload the following files:\")\n",
        "    print(\"1. test.jsonl\")\n",
        "    print(\"2. entity_lexicon.json\")\n",
        "    print(\"3. answers.txt\")\n",
        "    print()\n",
        "    uploaded = files.upload()\n",
        "    return uploaded\n",
        "\n",
        "def load_answers_from_file(file_path: str) -> List[int]:\n",
        "    \"\"\"Load answers from answers.txt file\"\"\"\n",
        "    answers = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.endswith('Yes'):\n",
        "                answers.append(1)\n",
        "            elif line.endswith('No'):\n",
        "                answers.append(0)\n",
        "    return answers\n",
        "\n",
        "def generate_ground_truth(entity_lexicon_path: str, test_jsonl_path: str, max_questions: int = 500) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"Generate the same questions and ground truth as the original script\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    with open(entity_lexicon_path, 'r') as f:\n",
        "        entity_lexicon = json.load(f)\n",
        "\n",
        "    test_examples = []\n",
        "    with open(test_jsonl_path, 'r') as f:\n",
        "        for line in f:\n",
        "            test_examples.append(json.loads(line))\n",
        "\n",
        "    questions = []\n",
        "    labels = []\n",
        "\n",
        "    for example in test_examples:\n",
        "        child_id = example[\"child\"]\n",
        "        parent_id = example[\"parent\"]\n",
        "\n",
        "        # Skip if entities not in lexicon\n",
        "        if child_id not in entity_lexicon or parent_id not in entity_lexicon:\n",
        "            continue\n",
        "\n",
        "        child_name = entity_lexicon[child_id][\"name\"]\n",
        "        parent_name = entity_lexicon[parent_id][\"name\"]\n",
        "\n",
        "        # Positive example (child -> parent)\n",
        "        question = f'Is \"{child_name}\" a subtype/subclass of \"{parent_name}\"?'\n",
        "        questions.append(question)\n",
        "        labels.append(1)  # Positive relationship\n",
        "\n",
        "        # Use all 10 negatives to match your actual test data\n",
        "        negatives = example.get(\"random_negatives\", [])[:10]  # All 10 negatives\n",
        "\n",
        "        for neg_id in negatives:\n",
        "            if neg_id in entity_lexicon:\n",
        "                neg_name = entity_lexicon[neg_id][\"name\"]\n",
        "                neg_question = f'Is \"{child_name}\" a subtype/subclass of \"{neg_name}\"?'\n",
        "                questions.append(neg_question)\n",
        "                labels.append(0)  # Negative relationship\n",
        "\n",
        "    # Shuffle to avoid patterns - use same seed for reproducibility\n",
        "    random.seed(42)\n",
        "    combined = list(zip(questions, labels))\n",
        "    random.shuffle(combined)\n",
        "    questions, labels = zip(*combined)\n",
        "\n",
        "    # Limit questions if specified\n",
        "    if max_questions and len(questions) > max_questions:\n",
        "        questions = questions[:max_questions]\n",
        "        labels = labels[:max_questions]\n",
        "\n",
        "    return list(questions), list(labels)\n",
        "\n",
        "def evaluate_llm_answers():\n",
        "    \"\"\"Main evaluation function\"\"\"\n",
        "\n",
        "    # Check if files exist, if not upload them\n",
        "    required_files = [\"test.jsonl\", \"entity_lexicon.json\", \"answers.txt\"]\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"Missing files: {missing_files}\")\n",
        "        print(\"Please upload the required files:\")\n",
        "        upload_files()\n",
        "\n",
        "    # File paths (in current directory)\n",
        "    entity_lexicon_path = \"entity_lexicon.json\"\n",
        "    test_jsonl_path = \"test.jsonl\"\n",
        "    answers_file = \"answers.txt\"\n",
        "\n",
        "    # Verify files exist\n",
        "    for file_path, name in [(entity_lexicon_path, \"entity_lexicon.json\"),\n",
        "                           (test_jsonl_path, \"test.jsonl\"),\n",
        "                           (answers_file, \"answers.txt\")]:\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {name}. Please upload it first.\")\n",
        "\n",
        "    # Load your answers\n",
        "    print(\"Loading LLM answers...\")\n",
        "    llm_predictions = load_answers_from_file(answers_file)\n",
        "    print(f\"Loaded {len(llm_predictions)} answers\")\n",
        "\n",
        "    # Generate ground truth (same order as original questions)\n",
        "    print(\"Generating ground truth...\")\n",
        "    questions, ground_truth = generate_ground_truth(entity_lexicon_path, test_jsonl_path, max_questions=500)\n",
        "    print(f\"Generated {len(ground_truth)} ground truth labels\")\n",
        "\n",
        "    # Ensure same length\n",
        "    min_len = min(len(llm_predictions), len(ground_truth))\n",
        "    predictions = llm_predictions[:min_len]\n",
        "    truth = ground_truth[:min_len]\n",
        "\n",
        "    print(f\"Evaluating on {min_len} questions...\")\n",
        "\n",
        "    if len(predictions) == 0:\n",
        "        raise ValueError(\"No valid predictions found. Check your answers.txt format.\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"total_questions\": len(truth),\n",
        "        \"answered_questions\": len(predictions),\n",
        "        \"f1_score\": f1_score(truth, predictions),\n",
        "        \"precision\": precision_score(truth, predictions, zero_division=0),\n",
        "        \"recall\": recall_score(truth, predictions, zero_division=0),\n",
        "        \"accuracy\": accuracy_score(truth, predictions),\n",
        "    }\n",
        "\n",
        "    # Additional analysis\n",
        "    positive_examples = sum(truth)\n",
        "    negative_examples = len(truth) - positive_examples\n",
        "\n",
        "    metrics.update({\n",
        "        \"positive_examples\": positive_examples,\n",
        "        \"negative_examples\": negative_examples,\n",
        "        \"predicted_positive\": sum(predictions),\n",
        "        \"predicted_negative\": len(predictions) - sum(predictions),\n",
        "    })\n",
        "\n",
        "    # Print results\n",
        "    print(\"=\"*60)\n",
        "    print(\"LLM EVALUATION RESULTS - DOID-MIXED DATASET\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total Questions: {metrics['total_questions']}\")\n",
        "    print(f\"Answered Questions: {metrics['answered_questions']}\")\n",
        "    print(f\"Answer Rate: {metrics['answered_questions']/metrics['total_questions']*100:.1f}%\")\n",
        "    print()\n",
        "    print(\"PERFORMANCE METRICS:\")\n",
        "    print(f\"F1 Score:    {metrics['f1_score']:.4f}\")\n",
        "    print(f\"Precision:   {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:      {metrics['recall']:.4f}\")\n",
        "    print(f\"Accuracy:    {metrics['accuracy']:.4f}\")\n",
        "    print()\n",
        "    print(\"DATA DISTRIBUTION:\")\n",
        "    print(f\"Positive Examples: {metrics['positive_examples']} ({metrics['positive_examples']/metrics['total_questions']*100:.1f}%)\")\n",
        "    print(f\"Negative Examples: {metrics['negative_examples']} ({metrics['negative_examples']/metrics['total_questions']*100:.1f}%)\")\n",
        "    print()\n",
        "    print(\"PREDICTION DISTRIBUTION:\")\n",
        "    print(f\"Predicted Positive: {metrics['predicted_positive']}\")\n",
        "    print(f\"Predicted Negative: {metrics['predicted_negative']}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Show some example misclassifications\n",
        "    print(\"\\nSAMPLE ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Find some misclassified examples\n",
        "    misclassified = []\n",
        "    for i, (pred, true) in enumerate(zip(predictions, truth)):\n",
        "        if pred != true and len(misclassified) < 5:\n",
        "            question = questions[i] if i < len(questions) else f\"Question {i+1}\"\n",
        "            misclassified.append((question, true, pred))\n",
        "\n",
        "    if misclassified:\n",
        "        print(\"Sample Misclassified Examples:\")\n",
        "        for question, true_label, pred_label in misclassified:\n",
        "            true_str = \"Yes\" if true_label == 1 else \"No\"\n",
        "            pred_str = \"Yes\" if pred_label == 1 else \"No\"\n",
        "            print(f\"Q: {question}\")\n",
        "            print(f\"   Ground Truth: {true_str}, Predicted: {pred_str}\")\n",
        "            print()\n",
        "\n",
        "    # Calculate confusion matrix manually\n",
        "    tp = sum(1 for p, t in zip(predictions, truth) if p == 1 and t == 1)\n",
        "    fp = sum(1 for p, t in zip(predictions, truth) if p == 1 and t == 0)\n",
        "    tn = sum(1 for p, t in zip(predictions, truth) if p == 0 and t == 0)\n",
        "    fn = sum(1 for p, t in zip(predictions, truth) if p == 0 and t == 1)\n",
        "\n",
        "    print(\"CONFUSION MATRIX:\")\n",
        "    print(f\"True Positives (TP):  {tp}\")\n",
        "    print(f\"False Positives (FP): {fp}\")\n",
        "    print(f\"True Negatives (TN):  {tn}\")\n",
        "    print(f\"False Negatives (FN): {fn}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def show_data_info():\n",
        "    \"\"\"Show information about the loaded data\"\"\"\n",
        "    if not all(os.path.exists(f) for f in [\"test.jsonl\", \"entity_lexicon.json\", \"answers.txt\"]):\n",
        "        print(\"Please run evaluate_llm_answers() first to load the data.\")\n",
        "        return\n",
        "\n",
        "    # Load and show basic info\n",
        "    with open(\"entity_lexicon.json\", 'r') as f:\n",
        "        entity_lexicon = json.load(f)\n",
        "\n",
        "    test_examples = []\n",
        "    with open(\"test.jsonl\", 'r') as f:\n",
        "        for line in f:\n",
        "            test_examples.append(json.loads(line))\n",
        "\n",
        "    print(\"DATASET INFORMATION:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total entities in lexicon: {len(entity_lexicon)}\")\n",
        "    print(f\"Total test examples: {len(test_examples)}\")\n",
        "\n",
        "    # Show sample entities\n",
        "    sample_entities = list(entity_lexicon.items())[:5]\n",
        "    print(\"\\nSample entities:\")\n",
        "    for uri, data in sample_entities:\n",
        "        print(f\"  {data.get('name', 'Unknown')}\")\n",
        "\n",
        "    # Show sample test example\n",
        "    if test_examples:\n",
        "        example = test_examples[0]\n",
        "        child_name = entity_lexicon.get(example['child'], {}).get('name', 'Unknown')\n",
        "        parent_name = entity_lexicon.get(example['parent'], {}).get('name', 'Unknown')\n",
        "        print(f\"\\nSample relationship:\")\n",
        "        print(f\"  Child: {child_name}\")\n",
        "        print(f\"  Parent: {parent_name}\")\n",
        "        print(f\"  Random negatives: {len(example.get('random_negatives', []))}\")\n",
        "        print(f\"  Hard negatives: {len(example.get('hard_negatives', []))}\")\n",
        "\n",
        "# Main execution\n",
        "print(\"ðŸ”¬ LLM Medical Ontology Evaluation Tool\")\n",
        "print(\"=\" * 50)\n",
        "print(\"This notebook evaluates LLM performance on hierarchical medical ontology relationships.\")\n",
        "print(\"\\nTo use:\")\n",
        "print(\"1. Upload your files: test.jsonl, entity_lexicon.json, answers.txt\")\n",
        "print(\"2. Run evaluate_llm_answers() to get evaluation results\")\n",
        "print(\"3. Run show_data_info() to see dataset information\")\n",
        "print(\"\\nReady to start! Run the functions below:\")\n",
        "\n",
        "# Uncomment the lines below to run automatically:\n",
        "# evaluate_llm_answers()\n",
        "# show_data_info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_llm_answers()\n",
        "show_data_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwtdTZN4fuAr",
        "outputId": "1df1491d-21db-4e3d-8aac-fbbb28469b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM answers...\n",
            "Loaded 500 answers\n",
            "Generating ground truth...\n",
            "Generated 500 ground truth labels\n",
            "Evaluating on 500 questions...\n",
            "============================================================\n",
            "LLM EVALUATION RESULTS - DOID-MIXED DATASET\n",
            "============================================================\n",
            "Total Questions: 500\n",
            "Answered Questions: 500\n",
            "Answer Rate: 100.0%\n",
            "\n",
            "PERFORMANCE METRICS:\n",
            "F1 Score:    0.7500\n",
            "Precision:   0.6964\n",
            "Recall:      0.8125\n",
            "Accuracy:    0.9480\n",
            "\n",
            "DATA DISTRIBUTION:\n",
            "Positive Examples: 48 (9.6%)\n",
            "Negative Examples: 452 (90.4%)\n",
            "\n",
            "PREDICTION DISTRIBUTION:\n",
            "Predicted Positive: 56\n",
            "Predicted Negative: 444\n",
            "============================================================\n",
            "\n",
            "SAMPLE ANALYSIS:\n",
            "----------------------------------------\n",
            "Sample Misclassified Examples:\n",
            "Q: Is \"shopping\" a subtype/subclass of \"score\"?\n",
            "   Ground Truth: No, Predicted: Yes\n",
            "\n",
            "Q: Is \"mat\" a subtype/subclass of \"branchiostegidae\"?\n",
            "   Ground Truth: No, Predicted: Yes\n",
            "\n",
            "Q: Is \"horsemeat\" a subtype/subclass of \"solid\"?\n",
            "   Ground Truth: Yes, Predicted: No\n",
            "\n",
            "Q: Is \"corer\" a subtype/subclass of \"war party\"?\n",
            "   Ground Truth: No, Predicted: Yes\n",
            "\n",
            "Q: Is \"shagginess\" a subtype/subclass of \"pledge\"?\n",
            "   Ground Truth: No, Predicted: Yes\n",
            "\n",
            "CONFUSION MATRIX:\n",
            "True Positives (TP):  39\n",
            "False Positives (FP): 17\n",
            "True Negatives (TN):  435\n",
            "False Negatives (FN): 9\n",
            "============================================================\n",
            "DATASET INFORMATION:\n",
            "==================================================\n",
            "Total entities in lexicon: 74401\n",
            "Total test examples: 33176\n",
            "\n",
            "Sample entities:\n",
            "  equinoctial point\n",
            "  autumnal equinox\n",
            "  anteater\n",
            "  tamandua\n",
            "  military uniform\n",
            "\n",
            "Sample relationship:\n",
            "  Child: sensation\n",
            "  Parent: perception\n",
            "  Random negatives: 10\n",
            "  Hard negatives: 10\n"
          ]
        }
      ]
    }
  ]
}